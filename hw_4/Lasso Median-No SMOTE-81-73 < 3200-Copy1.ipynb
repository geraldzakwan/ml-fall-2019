{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_excluded_features():\n",
    "    df = pd.read_csv('CodeBook-SELECT.csv')\n",
    "    excluded = []\n",
    "\n",
    "#     for i in range(0, 377):\n",
    "#         desc = df.iloc[i]['Description']\n",
    "#         varname = df.iloc[i]['VarName']\n",
    "\n",
    "#         if 'ISCO' in desc or 'ISCO' in varname:\n",
    "#             excluded.append(varname)\n",
    "\n",
    "#         elif 'ISIC' in desc or 'ISIC' in varname:\n",
    "#             excluded.append(varname)\n",
    "\n",
    "#         elif 'mth' in desc or 'mth' in varname:\n",
    "#             excluded.append(varname)\n",
    "\n",
    "#         elif 'coded' in desc or 'coded' in varname:\n",
    "#             excluded.append(varname)\n",
    "\n",
    "    return excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_numeric_columns(df):\n",
    "    # Find numeric columns\n",
    "    num_col = []\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            num_col.append(col)\n",
    "\n",
    "    print('NUM COL')\n",
    "    print(len(num_col))\n",
    "\n",
    "    incl_num_cols = []\n",
    "    for col in num_col:\n",
    "        noise = [9996, 9997, 9998, 9999, 996, 997, 998, 999]\n",
    "        df[col] = df[col].replace(noise, np.nan)\n",
    "                \n",
    "        if df[col].isna().sum() < 10000:\n",
    "            m = df[col].mean()\n",
    "            if m > 1990.0 and m < 2020.0:\n",
    "                df[col] = df[col] - 1990.0\n",
    "            incl_num_cols.append(col)\n",
    "#             if m >= 1.0:\n",
    "#                 if not (m > 9000.0 and m < 10000.0):\n",
    "#                     incl_num_cols.append(col)\n",
    "\n",
    "#                     if m > 1990.0 and m < 2020.0:\n",
    "#                         df[col] = df[col] - 1990.0\n",
    "                        \n",
    "#     df[incl_num_cols] = df[incl_num_cols].fillna(df.median().iloc[0])\n",
    "#     df[incl_num_cols] = df[incl_num_cols].fillna(df[incl_num_cols].median().iloc[0])\n",
    "    df[incl_num_cols] = df[incl_num_cols].fillna(df[incl_num_cols].mean().iloc[0])\n",
    "    print('ASU 1')\n",
    "    print(df[incl_num_cols].isnull().values.any())\n",
    "#     df[incl_num_cols] = df[incl_num_cols].fillna(df[incl_num_cols].mode())\n",
    "\n",
    "    print('INCL COL')\n",
    "    print(len(incl_num_cols))\n",
    "\n",
    "    return df, incl_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, col_names):\n",
    "    print(len(col_names))\n",
    "\n",
    "    iter = 0\n",
    "    new_cols = []\n",
    "    for col in col_names:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            if len(df[col].unique()) < 3:\n",
    "                df[col] = df[col].astype('category')\n",
    "                df[col] = df[col].cat.codes\n",
    "                new_cols.append(col)\n",
    "            else:\n",
    "                dummies = pd.get_dummies(df[col],prefix=col)\n",
    "                new_cols = new_cols + dummies.columns.tolist()\n",
    "                df = pd.concat([df,dummies],axis=1)\n",
    "                #drop the encoded column\n",
    "                df.drop([col],axis = 1 , inplace=True)\n",
    "\n",
    "            iter = iter + 1\n",
    "            if (iter % 100 == 0):\n",
    "                print(iter)\n",
    "\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_cat_cols(df, incl_num_cols):\n",
    "    # TODO: Remove 999?\n",
    "\n",
    "    cat_col = list(set(df.columns).difference(set(incl_num_cols)))\n",
    "\n",
    "    print('CAT COL')\n",
    "    print(len(cat_col))\n",
    "\n",
    "    incl_cat_cols = []\n",
    "    for col in cat_col:\n",
    "        noise = ['9996', '9997', '9998', '9999', '996', '997', '998', '999']\n",
    "        df[col] = df[col].replace(noise, np.nan)\n",
    "        \n",
    "        incl_cat_cols.append(col)\n",
    "        \n",
    "#         # INI PENTING BGT, KALO NGK JADI MINUS 900 MEANNYA WKWK\n",
    "#         if 'v' in col and col != 'vet' and len(col) < 5:\n",
    "#             if len(df[col].unique()) < 51:\n",
    "#                 incl_cat_cols.append(col)\n",
    "#         else:\n",
    "#             incl_cat_cols.append(col)\n",
    "#         incl_cat_cols.append(col)\n",
    "    # for col in cat_col:\n",
    "    #     if len(df[col].unique()) < 11:\n",
    "    #             incl_cat_cols.append(col)\n",
    "    #     incl_cat_cols.append(col)\n",
    "    \n",
    "#     df[incl_cat_cols] = df[incl_cat_cols].fillna(df.mode().iloc[0])\n",
    "    df[incl_cat_cols] = df[incl_cat_cols].fillna(df[incl_cat_cols].mode().iloc[0])\n",
    "\n",
    "    print('INCL COL')\n",
    "    print(len(incl_num_cols))\n",
    "\n",
    "    print('There were {} columns before encoding categorical features'.format(df.shape[1]))\n",
    "    df, incl_cat_cols = one_hot_encode(df, incl_cat_cols)\n",
    "    print('There are {} columns after encoding categorical features'.format(df.shape[1]))\n",
    "\n",
    "    return df, incl_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(df, incl_num_cols, incl_cat_cols):\n",
    "    # Drop 40% of the males to obtain balance\n",
    "#     sampling_percentage = 0\n",
    "    sampling_percentage = 40\n",
    "#     dropped_indexes = df[df['gender_r_Male'] == 1].sample(frac=float(sampling_percentage/100), random_state = 28).index\n",
    "    dropped_indexes = df[df['gender_r'] == 1].sample(frac=float(sampling_percentage/100), random_state = 28).index\n",
    "\n",
    "    with open('dropped_indexes_' + str(sampling_percentage) + '.pickle', 'wb') as outfile:\n",
    "        # dump information to that file\n",
    "        pickle.dump(dropped_indexes, outfile)\n",
    "\n",
    "    train_df = df.drop(dropped_indexes)\n",
    "    print(len(train_df))\n",
    "    print(len(train_df.columns))\n",
    "    train_df = train_df[incl_num_cols + incl_cat_cols]\n",
    "\n",
    "    df = df[incl_num_cols + incl_cat_cols]\n",
    "    print(len(df))\n",
    "    print(len(df.columns))\n",
    "\n",
    "    return df, train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "def train_and_eval(df, train_df):\n",
    "    X_train = train_df.drop(['job_performance'], axis=1).values\n",
    "    y_train = train_df['job_performance'].values\n",
    "\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    # # Create the parameter grid based on the results of random search\n",
    "    # param_grid = {\n",
    "    #     'bootstrap': [True],\n",
    "    #     'max_depth': [10, 20],\n",
    "    #     'max_features': ['auto'],\n",
    "    #     'min_samples_leaf': [50, 100],\n",
    "    #     'min_samples_split': [100, 200],\n",
    "    #     'n_estimators': [20, 50]\n",
    "    # }\n",
    "    # # Create a based model\n",
    "    # rf = RandomForestRegressor(random_state = 40)\n",
    "    # # Instantiate the grid search model\n",
    "    # grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 2)\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Create the parameter grid based on the results of random search\n",
    "#     param_grid = {\n",
    "#         'alpha': [0.1, 0.2, 0.5, 1.0]\n",
    "#     }\n",
    "#     # Create a based model\n",
    "#     rf = linear_model.Lasso(random_state = 28)\n",
    "#     # Instantiate the grid search model\n",
    "#     grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     print(grid_search.best_params_)\n",
    "#     clf = grid_search.best_estimator_\n",
    "    \n",
    "    # can be any estimator that has attribute 'feature_importances_' or 'coef_'\n",
    "#     model = RandomForestRegressor(random_state=28) \n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     fs = SelectFromModel(model, prefit=True)\n",
    "\n",
    "#     X_train_new = fs.transform(X_train) # columns selected\n",
    "    \n",
    "#     print(len(X_train_new[0]))\n",
    "#     print(X_train_new.columns)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "#     clf = RandomForestRegressor(max_depth = 20, min_samples_leaf = 25, min_samples_split = 50, n_estimators = 100, random_state = 28)\n",
    "#     clf = linear_model.LinearRegression()\n",
    "#     clf = linear_model.Lasso(alpha=1.0, max_iter=1000)\n",
    "    clf = linear_model.Lasso(alpha=0.1, max_iter=1000)\n",
    "#     clf = linear_model.Ridge(alpha=1.0)\n",
    "#     clf = linear_model.Ridge(alpha=1.0)\n",
    "    # {'bootstrap': True, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 50}\n",
    "#     print(clf.get_params())\n",
    "#     clf = linear_model.LinearRegression()\n",
    "#     scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    scores = cross_validate(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    print(scores)\n",
    "    \n",
    "#     clf.fit(X_train_new, y_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "#     X_train = fs.transform(df.drop(['job_performance'], axis=1).values)\n",
    "    X_train = df.drop(['job_performance'], axis=1).values\n",
    "    y_train = df['job_performance'].values\n",
    "\n",
    "    y_pred = clf.predict(X_train)\n",
    "\n",
    "    print('Mean: ' + str(np.mean(y_pred)))\n",
    "    print('Median: ' + str(np.median(y_pred)))\n",
    "    print('Variance: ' + str(np.var(y_pred)))\n",
    "\n",
    "    return clf, y_pred, mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_1(model_name):\n",
    "    df = pd.read_csv('hw4-trainingset-gd2551.csv')\n",
    "    df = df.drop(['uni', 'row'] + get_excluded_features(), axis=1)\n",
    "\n",
    "    # Impute columns simply with mode\n",
    "#     df = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "    df, incl_num_cols = get_included_numeric_columns(df)\n",
    "\n",
    "    df, incl_cat_cols = get_included_cat_cols(df, incl_num_cols)\n",
    "\n",
    "    df, train_df = prepare_train(df, incl_num_cols, incl_cat_cols)\n",
    "\n",
    "    clf, df['y_pred'], mse = train_and_eval(df, train_df)\n",
    "\n",
    "    print('MSE: ' + str(mse))\n",
    "\n",
    "    with open(model_name, 'wb') as outfile:\n",
    "        pickle.dump(clf, outfile)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_1('apa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_2(model_name, train_cols):\n",
    "    df = pd.read_csv('hw4-testset-gd2551.csv')\n",
    "    df = df.drop(['uni', 'row'] + get_excluded_features(), axis=1)\n",
    "\n",
    "    # Impute columns simply with mode\n",
    "#     df = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "    df, incl_num_cols = get_included_numeric_columns(df)\n",
    "\n",
    "    df, incl_cat_cols = get_included_cat_cols(df, incl_num_cols)\n",
    "\n",
    "    for missing_col in list(set(train_cols).difference(set(df.columns))):\n",
    "        df[missing_col] = np.zeros(24500, dtype='int')\n",
    "\n",
    "    df = df[train_cols]\n",
    "    \n",
    "#     for gajelas in ['v272', 'v52', 'v135']:\n",
    "#         df[gajelas] = np.zeros(24500, dtype='int')\n",
    "\n",
    "    X_test = df.drop(['job_performance'], axis=1).values\n",
    "    print(np.where(np.isnan(X_test)))\n",
    "    \n",
    "    for idx in set(np.where(np.isnan(X_test))[1]):\n",
    "        X_test[:, idx] = np.zeros(24500, dtype='int')\n",
    "\n",
    "    with open(model_name, 'rb') as infile:\n",
    "        clf = pickle.load(infile)\n",
    "\n",
    "    df['job_performance'] = clf.predict(X_test)\n",
    "\n",
    "    print('Mean: ' + str(np.mean(df['job_performance'])))\n",
    "    print('Median: ' + str(np.median(df['job_performance'])))\n",
    "    print('Variance: ' + str(np.var(df['job_performance'])))\n",
    "\n",
    "    return df, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_2('test_model.pickle', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3209: DtypeWarning: Columns (50,172,255,256,257,258,268,280,376) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM COL\n",
      "84\n",
      "ASU 1\n",
      "False\n",
      "INCL COL\n",
      "32\n",
      "CAT COL\n",
      "346\n",
      "INCL COL\n",
      "32\n",
      "There were 378 columns before encoding categorical features\n",
      "346\n",
      "100\n",
      "200\n",
      "There are 2921 columns after encoding categorical features\n",
      "15002\n",
      "2921\n",
      "20000\n",
      "2868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 486422901.87014186, tolerance: 246799.12205644394\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484019594.9790845, tolerance: 248250.08127650377\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484572559.88172853, tolerance: 247226.1494313713\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 483857528.51628256, tolerance: 245796.8244111192\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 481823617.75587094, tolerance: 247037.96785240827\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479574887.7640783, tolerance: 245986.2750462144\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 488151666.3231404, tolerance: 247055.05569535156\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 486087647.78781664, tolerance: 246040.20965280916\n",
      "  positive)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 483630002.9383757, tolerance: 246132.4404141075\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "x = main_1('test_model.pickle')\n",
    "# print('NUMS')\n",
    "#     print(set(x).difference(set(a)))\n",
    "# print('----')\n",
    "# print(set(a).difference(set(x)))\n",
    "# print('----')\n",
    "#\n",
    "# print('CATS')\n",
    "# print(set(y).difference(set(b)))\n",
    "# print('----')\n",
    "# print(set(b).difference(set(y)))\n",
    "# print('----')\n",
    "\n",
    "%matplotlib inline\n",
    "x.hist(column='job_performance')\n",
    "x.hist(column='y_pred')\n",
    "# len(x[x['gender_r'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[x['gender_r_Male'] == 0].hist(column='job_performance')\n",
    "# x[x['gender_r_Male'] == 0].hist(column='y_pred')\n",
    "x[x['gender_r'] == 0].hist(column='job_performance')\n",
    "x[x['gender_r'] == 0].hist(column='y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[x['gender_r_Male'] == 1].hist(column='job_performance')\n",
    "# x[x['gender_r_Male'] == 1].hist(column='y_pred')\n",
    "x[x['gender_r'] == 1].hist(column='job_performance')\n",
    "x[x['gender_r'] == 1].hist(column='y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "a_df, a_x_test = main_2('test_model.pickle', x.drop(['y_pred'], axis=1).columns)\n",
    "a_df.hist('job_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a_df.columns[[19,20,22]])\n",
    "# a_x_test[:, 22]\n",
    "# print(np.count_nonzero(~np.isnan(a_x_test[:, 19])))\n",
    "# print(np.count_nonzero(~np.isnan(a_x_test[:, 20])))\n",
    "# print(np.count_nonzero(~np.isnan(a_x_test[:, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_df[a_df['gender_r_Male'] == 0].hist(column='job_performance')\n",
    "a_df[a_df['gender_r'] == 0].hist(column='job_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_df[a_df['gender_r_Male'] == 1].hist(column='job_performance')\n",
    "a_df[a_df['gender_r'] == 1].hist(column='job_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MEDIAN and SCALER\n",
    "# kalo bisa regression bgs\n",
    "# Insight: LASSO BETTER MEAN alpha 0.1 bgs dr 0.25\n",
    "\n",
    "# LINREG FIX CORET, MEAN KECIL 1700\n",
    "# RIDGE 0.5 JG SAMA 2100\n",
    "# RIDGE 1.0 2300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_df[a_df['job_performance'] <= 3200]) / 24500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x[x['job_performance'] <= 3400]) / 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x[x['y_pred'] <= 3200]) / 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(a_df[a_df['job_performance'] <= 1500]['gender_r_Female'])\n",
    "len(a_df[a_df['job_performance'] <= 1500]['gender_r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x[x['job_performance'] <= 1500]['gender_r_Female'])\n",
    "# len(x[x['job_performance'] <= 1500]['gender_r'])\n",
    "dropped_indexes = x[x['gender_r'] == 1].sample(frac=float(40/100), random_state = 28).index\n",
    "y = x.drop(dropped_indexes)\n",
    "print(len(y[y['job_performance'] <= 3200]) / len(y))\n",
    "\n",
    "print(x['job_performance'].mean())\n",
    "print(x['job_performance'].median())\n",
    "print(x['job_performance'].var())\n",
    "\n",
    "print(y['job_performance'].mean())\n",
    "print(y['job_performance'].median())\n",
    "print(y['job_performance'].var())\n",
    "\n",
    "print(len(y))\n",
    "print(len(y[y['gender_r'] == 0]))\n",
    "print(len(y[y['gender_r'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hw4-trainingset-gd2551.csv')\n",
    "df = df.drop(['uni', 'row'] + get_excluded_features(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_col = []\n",
    "# for col in df.columns:\n",
    "#     if np.issubdtype(df[col].dtype, np.number):\n",
    "#         num_col.append(col)\n",
    "\n",
    "# print('NUM COL')\n",
    "# print(len(num_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[num_col].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean train: ' + str(df['job_performance'].mean()))\n",
    "print('Median train: ' + str(df['job_performance'].median()))\n",
    "print('Mean male: ' + str(df[df['gender_r'] == 'Male']['job_performance'].mean()))\n",
    "print('Median male: ' + str(df[df['gender_r'] == 'Male']['job_performance'].median()))\n",
    "print('Mean female: ' + str(df[df['gender_r'] == 'Female']['job_performance'].mean()))\n",
    "print('Median female: ' + str(df[df['gender_r'] == 'Female']['job_performance'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean train: ' + str(x['y_pred'].mean()))\n",
    "print('Median train: ' + str(x['y_pred'].median()))\n",
    "print('Mean male: ' + str(x[x['gender_r'] == 1]['y_pred'].mean()))\n",
    "print('Median male: ' + str(x[x['gender_r'] == 1]['y_pred'].median()))\n",
    "print('Mean female: ' + str(x[x['gender_r'] == 0]['y_pred'].mean()))\n",
    "print('Median female: ' + str(x[x['gender_r'] == 0]['y_pred'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean train: ' + str(a_df['job_performance'].mean()))\n",
    "print('Median train: ' + str(a_df['job_performance'].median()))\n",
    "print('Mean male: ' + str(a_df[a_df['gender_r'] == 1]['job_performance'].mean()))\n",
    "print('Median male: ' + str(a_df[a_df['gender_r'] == 1]['job_performance'].median()))\n",
    "print('Mean female: ' + str(a_df[a_df['gender_r'] == 0]['job_performance'].mean()))\n",
    "print('Median female: ' + str(a_df[a_df['gender_r'] == 0]['job_performance'].median()))\n",
    "\n",
    "# MEAN NYA RENDAH BGT KALO GA DI SAMPLING JADI 2000 BAWAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['job_performance'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['nfehrs'].max())\n",
    "# print(df[df['nfehrs'] >= 1000.0]['nfehrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['v63'].mean())\n",
    "# print(df[df['v63'] >= 1000.0]['v63'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Submission\n",
    "# final_df = pd.read_csv('hw4-testset-gd2551.csv')\n",
    "# final_df['job_performance'] = a_df['job_performance']\n",
    "# final_df.to_csv(index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
