{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_excluded_features():\n",
    "    df = pd.read_csv('CodeBook-SELECT.csv')\n",
    "    excluded = []\n",
    "\n",
    "    for i in range(0, 377):\n",
    "        desc = df.iloc[i]['Description']\n",
    "        varname = df.iloc[i]['VarName']\n",
    "\n",
    "        if 'ISCO' in desc or 'ISCO' in varname:\n",
    "            excluded.append(varname)\n",
    "\n",
    "        elif 'ISIC' in desc or 'ISIC' in varname:\n",
    "            excluded.append(varname)\n",
    "\n",
    "        elif 'mth' in desc or 'mth' in varname:\n",
    "            excluded.append(varname)\n",
    "\n",
    "        elif 'coded' in desc or 'coded' in varname:\n",
    "            excluded.append(varname)\n",
    "\n",
    "    return excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_numeric_columns(df):\n",
    "    # Find numeric columns\n",
    "    num_col = []\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            num_col.append(col)\n",
    "\n",
    "    print('NUM COL')\n",
    "    print(len(num_col))\n",
    "\n",
    "    incl_num_cols = []\n",
    "    for col in num_col:\n",
    "        if not df[col].isna().sum() > 10000:\n",
    "            m = df[col].mean()\n",
    "            if m >= 1.0:\n",
    "                if not (m > 9000.0 and m < 10000.0):\n",
    "                    incl_num_cols.append(col)\n",
    "\n",
    "                    if m > 1990.0 and m < 2020.0:\n",
    "                        df[col] = df[col] - 1990.0\n",
    "                        \n",
    "#     df[incl_num_cols] = df[incl_num_cols].fillna(df.median().iloc[0])\n",
    "    df[incl_num_cols] = df[incl_num_cols].fillna(df[incl_num_cols].median().iloc[0])\n",
    "    print('ASU 1')\n",
    "    print(df[incl_num_cols].isnull().values.any())\n",
    "    \n",
    "    print('VAR')\n",
    "    var_list = df[incl_num_cols].var()\n",
    "#     df[incl_num_cols] = df[incl_num_cols].fillna(df[incl_num_cols].mode())\n",
    "\n",
    "    print('INCL COL')\n",
    "    print(len(incl_num_cols))\n",
    "    \n",
    "    return var_list\n",
    "\n",
    "    return df, incl_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, col_names):\n",
    "    print(len(col_names))\n",
    "\n",
    "    iter = 0\n",
    "    new_cols = []\n",
    "    for col in col_names:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            new_cols = new_cols + dummies.columns.tolist()\n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "            #drop the encoded column\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "\n",
    "            iter = iter + 1\n",
    "            if (iter % 100 == 0):\n",
    "                print(iter)\n",
    "\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_included_cat_cols(df, incl_num_cols):\n",
    "    # TODO: Remove 999?\n",
    "\n",
    "    cat_col = list(set(df.columns).difference(set(incl_num_cols)))\n",
    "\n",
    "    print('CAT COL')\n",
    "    print(len(cat_col))\n",
    "\n",
    "    incl_cat_cols = []\n",
    "    for col in cat_col:\n",
    "        if 'v' in col and col != 'vet' and len(col) < 5:\n",
    "            if len(df[col].unique()) < 11:\n",
    "                incl_cat_cols.append(col)\n",
    "        else:\n",
    "            incl_cat_cols.append(col)\n",
    "    # for col in cat_col:\n",
    "    #     if len(df[col].unique()) < 11:\n",
    "    #             incl_cat_cols.append(col)\n",
    "    #     incl_cat_cols.append(col)\n",
    "    \n",
    "#     df[incl_cat_cols] = df[incl_cat_cols].fillna(df.mode().iloc[0])\n",
    "    df[incl_cat_cols] = df[incl_cat_cols].fillna(df[incl_cat_cols].mode().iloc[0])\n",
    "\n",
    "    print('INCL COL')\n",
    "    print(len(incl_num_cols))\n",
    "\n",
    "    print('There were {} columns before encoding categorical features'.format(df.shape[1]))\n",
    "    df, incl_cat_cols = one_hot_encode(df, incl_cat_cols)\n",
    "    print('There are {} columns after encoding categorical features'.format(df.shape[1]))\n",
    "\n",
    "    return df, incl_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(df, incl_num_cols, incl_cat_cols):\n",
    "    # Drop 40% of the males to obtain balance\n",
    "    sampling_percentage = 0\n",
    "#     sampling_percentage = 40\n",
    "    dropped_indexes = df[df['gender_r_Male'] == 1].sample(frac=float(sampling_percentage/100), random_state = 28).index\n",
    "\n",
    "    with open('dropped_indexes_' + str(sampling_percentage) + '.pickle', 'wb') as outfile:\n",
    "        # dump information to that file\n",
    "        pickle.dump(dropped_indexes, outfile)\n",
    "\n",
    "    train_df = df.drop(dropped_indexes)\n",
    "    print(len(train_df))\n",
    "    print(len(train_df.columns))\n",
    "    train_df = train_df[incl_num_cols + incl_cat_cols]\n",
    "\n",
    "    df = df[incl_num_cols + incl_cat_cols]\n",
    "    print(len(df))\n",
    "    print(len(df.columns))\n",
    "\n",
    "    return df, train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "def train_and_eval(df, train_df):\n",
    "    X_train = train_df.drop(['job_performance'], axis=1).values\n",
    "    y_train = train_df['job_performance'].values\n",
    "\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    # # Create the parameter grid based on the results of random search\n",
    "    # param_grid = {\n",
    "    #     'bootstrap': [True],\n",
    "    #     'max_depth': [10, 20],\n",
    "    #     'max_features': ['auto'],\n",
    "    #     'min_samples_leaf': [50, 100],\n",
    "    #     'min_samples_split': [100, 200],\n",
    "    #     'n_estimators': [20, 50]\n",
    "    # }\n",
    "    # # Create a based model\n",
    "    # rf = RandomForestRegressor(random_state = 40)\n",
    "    # # Instantiate the grid search model\n",
    "    # grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 2)\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # print(grid_search.best_params_)\n",
    "    # clf = grid_search.best_estimator_\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    clf = RandomForestRegressor(max_depth = 20, min_samples_leaf = 25, min_samples_split = 50, n_estimators = 200, random_state = 28)\n",
    "#     clf = linear_model.LinearRegression()\n",
    "#     clf = linear_model.Lasso(alpha=0.1)\n",
    "    # {'bootstrap': True, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 50, 'min_samples_split': 100, 'n_estimators': 50}\n",
    "#     print(clf.get_params())\n",
    "#     clf = linear_model.LinearRegression()\n",
    "#     scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    scores = cross_validate(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    print(scores)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    X_train = df.drop(['job_performance'], axis=1).values\n",
    "    y_train = df['job_performance'].values\n",
    "\n",
    "    y_pred = clf.predict(X_train)\n",
    "\n",
    "    print('Mean: ' + str(np.mean(y_pred)))\n",
    "    print('Variance: ' + str(np.var(y_pred)))\n",
    "\n",
    "    return clf, y_pred, mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_1(model_name):\n",
    "    df = pd.read_csv('hw4-trainingset-gd2551.csv')\n",
    "    df = df.drop(['uni', 'row'] + get_excluded_features(), axis=1)\n",
    "\n",
    "    # Impute columns simply with mode\n",
    "#     df = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "    df = get_included_numeric_columns(df)\n",
    "\n",
    "#     df, incl_cat_cols = get_included_cat_cols(df, incl_num_cols)\n",
    "\n",
    "#     df, train_df = prepare_train(df, incl_num_cols, incl_cat_cols)\n",
    "\n",
    "#     clf, df['y_pred'], mse = train_and_eval(df, train_df)\n",
    "\n",
    "#     print('MSE: ' + str(mse))\n",
    "\n",
    "#     with open(model_name, 'wb') as outfile:\n",
    "#         pickle.dump(clf, outfile)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM COL\n",
      "60\n",
      "ASU 1\n",
      "False\n",
      "VAR\n",
      "INCL COL\n",
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age_r                  73.802227\n",
       "yrsqual                68.091451\n",
       "yrsget                 99.589107\n",
       "leavedu                71.861939\n",
       "nfehrs              53152.042298\n",
       "learnatwork           146.478051\n",
       "readytolearn            2.266581\n",
       "icthome                85.367297\n",
       "ictwork                87.586475\n",
       "influence              17.197670\n",
       "planning               28.395500\n",
       "readhome                4.076244\n",
       "readwork                2.008731\n",
       "taskdisc               73.793250\n",
       "writhome               57.955281\n",
       "writwork               24.203411\n",
       "job_performance    183187.685640\n",
       "v202                   75.352066\n",
       "v231                   75.170998\n",
       "v272                  344.581539\n",
       "v52                   180.247557\n",
       "v33                   309.953057\n",
       "v135                  142.928223\n",
       "v63                606403.125523\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_var = main_1('apa')\n",
    "df_var\n",
    "# len(df_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_2(model_name, train_cols):\n",
    "    df = pd.read_csv('hw4-testset-gd2551.csv')\n",
    "    df = df.drop(['uni', 'row'] + get_excluded_features(), axis=1)\n",
    "\n",
    "    # Impute columns simply with mode\n",
    "#     df = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "    df, incl_num_cols = get_included_numeric_columns(df)\n",
    "\n",
    "    df, incl_cat_cols = get_included_cat_cols(df, incl_num_cols)\n",
    "\n",
    "    for missing_col in list(set(train_cols).difference(set(df.columns))):\n",
    "        df[missing_col] = np.zeros(24500, dtype='int')\n",
    "\n",
    "    df = df[train_cols]\n",
    "    \n",
    "#     for gajelas in ['v272', 'v52', 'v135']:\n",
    "#         df[gajelas] = np.zeros(24500, dtype='int')\n",
    "\n",
    "    X_test = df.drop(['job_performance'], axis=1).values\n",
    "    print(np.where(np.isnan(X_test)))\n",
    "    \n",
    "    for idx in set(np.where(np.isnan(X_test))[1]):\n",
    "        X_test[:, idx] = np.zeros(24500, dtype='int')\n",
    "\n",
    "    with open(model_name, 'rb') as infile:\n",
    "        clf = pickle.load(infile)\n",
    "\n",
    "    df['job_performance'] = clf.predict(X_test)\n",
    "\n",
    "    print('Mean: ' + str(np.mean(df['job_performance'])))\n",
    "    print('Variance: ' + str(np.var(df['job_performance'])))\n",
    "\n",
    "    return df, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_2('test_model.pickle', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3209: DtypeWarning: Columns (50,172,255,256,257,258,268,280,376) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM COL\n",
      "60\n",
      "ASU 1\n",
      "False\n",
      "INCL COL\n",
      "24\n",
      "CAT COL\n",
      "319\n",
      "INCL COL\n",
      "24\n",
      "There were 343 columns before encoding categorical features\n",
      "277\n",
      "100\n",
      "200\n",
      "There are 1249 columns after encoding categorical features\n",
      "20000\n",
      "1249\n",
      "20000\n",
      "1196\n"
     ]
    }
   ],
   "source": [
    "x = main_1('test_model.pickle')\n",
    "# print('NUMS')\n",
    "#     print(set(x).difference(set(a)))\n",
    "# print('----')\n",
    "# print(set(a).difference(set(x)))\n",
    "# print('----')\n",
    "#\n",
    "# print('CATS')\n",
    "# print(set(y).difference(set(b)))\n",
    "# print('----')\n",
    "# print(set(b).difference(set(y)))\n",
    "# print('----')\n",
    "\n",
    "%matplotlib inline\n",
    "x.hist(column='job_performance')\n",
    "x.hist(column='y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x['gender_r_Male'] == 0].hist(column='job_performance')\n",
    "x[x['gender_r_Male'] == 0].hist(column='y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x['gender_r_Male'] == 1].hist(column='job_performance')\n",
    "x[x['gender_r_Male'] == 1].hist(column='y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "a_df, a_x_test = main_2('test_model.pickle', x.drop(['y_pred'], axis=1).columns)\n",
    "a_df.hist('job_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a_df.columns[[19,20,22]])\n",
    "# a_x_test[:, 22]\n",
    "# print(np.count_nonzero(~np.isnan(a_x_test[:, 19])))\n",
    "# print(np.count_nonzero(~np.isnan(a_x_test[:, 20])))\n",
    "# print(np.count_nonzero(~np.isnan(a_x_test[:, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df[a_df['gender_r_Male'] == 0].hist(column='job_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df[a_df['gender_r_Male'] == 1].hist(column='job_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MEDIAN and SCALER\n",
    "# kalo bisa regression bgs\n",
    "# Insight: LASSO BETTER MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_df[a_df['job_performance'] <= 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x[x['job_performance'] <= 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
