{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def load_news_data(filepath):\n",
    "    news = loadmat(filepath)\n",
    "\n",
    "    # From scipy csc matrix to 2d ndarray\n",
    "    train_data = news['data'].toarray()\n",
    "    # From 2d ndarray to 1d ndarray\n",
    "    train_labels = news['labels'].flatten()\n",
    "\n",
    "    test_data = news['testdata'].toarray()\n",
    "    test_labels = news['testlabels'].flatten()\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def create_dictionary(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        list_of_words = f.readlines()\n",
    "\n",
    "    return list_of_words\n",
    "\n",
    "# This is going to be the pi_y\n",
    "def calculate_label_count_and_probability(labels):\n",
    "    label_count = defaultdict(int)\n",
    "\n",
    "    for label in labels:\n",
    "        label_count[label] = label_count[label] + 1\n",
    "\n",
    "    label_probability = {}\n",
    "\n",
    "    for label in labels:\n",
    "        label_probability[label] = label_count[label] / len(labels)\n",
    "\n",
    "    return label_count, label_probability\n",
    "\n",
    "def check_sum_probability(collection):\n",
    "    sum_of_probs = 0\n",
    "\n",
    "    if isinstance(collection, dict):\n",
    "        for label in label_probability:\n",
    "            sum_of_probs = sum_of_probs + label_probability[label]\n",
    "    else:\n",
    "#         print('MASUK LIST')\n",
    "        sum_of_probs = np.sum(collection)\n",
    "\n",
    "    print(sum_of_probs)\n",
    "    \n",
    "    return abs(sum_of_probs - 1.0) < 0.001\n",
    "\n",
    "# This is going to be miu_y_j\n",
    "# The idea is to calculate separately for each value of y\n",
    "# This is done by getting train_data only for a particular value of y\n",
    "# Then, sum over the word index and divide by the number of data\n",
    "def calculate_word_given_label_probability(train_data, train_labels, word_list):\n",
    "    label_count, label_probability = calculate_label_count_and_probability(train_labels)\n",
    "    \n",
    "    # Return array of shape(20, 61188)\n",
    "    ret_arr = np.zeros((len(label_count), len(word_list)))\n",
    "    \n",
    "    # Iterate over label\n",
    "    for label in label_count:\n",
    "        # GOBLOK ANJING TADI train_labels == 1 WOKWOKWOK\n",
    "        idx, = np.where(train_labels == label)\n",
    "        \n",
    "        first_idx = idx[0]\n",
    "        last_idx = idx[-1]\n",
    "        \n",
    "#         print(train_data.shape)\n",
    "        \n",
    "        # Get the corresponding train_data\n",
    "        corr_train_data = train_data[first_idx:last_idx]\n",
    "#         print(corr_train_data.shape)\n",
    "        \n",
    "        # Sum over axis=0, i.e. sum word occurrence\n",
    "        word_sum = np.sum(corr_train_data, axis=0)\n",
    "#         print(word_sum.shape)\n",
    "        word_sum = np.add(word_sum, 1)\n",
    "#         print(np.max(word_sum) < label_count[label] + 2)\n",
    "        \n",
    "        # Divide by label_count + 2 (laplace_smoothing)\n",
    "#         print(label_count[label])\n",
    "#         word_prob = np.divide(word_sum, label_count[label]) \n",
    "        word_prob = np.divide(word_sum, label_count[label] + 2) \n",
    "#         print(word_prob[0:20])\n",
    "        \n",
    "        # Assign to ret_arr\n",
    "        ret_arr[label - 1] = word_prob\n",
    "        \n",
    "    return ret_arr\n",
    "\n",
    "# Check if for all j (words), miu_y_j sums up to 1\n",
    "# def check_word_probability(word_prob):\n",
    "#     sum_over_word_probs = np.sum(word_prob, axis=0)\n",
    "    \n",
    "#     print(sum_over_word_probs)\n",
    "    \n",
    "#     for elem in sum_over_word_probs:\n",
    "#         if not (abs(elem - 1.0) < 0.001):\n",
    "#             return False\n",
    "\n",
    "#     return True\n",
    "\n",
    "# This is going to be the P(X|Y)\n",
    "def predict(feature_vector, word_prob, train_labels):\n",
    "    label_count, label_probability = calculate_label_count_and_probability(train_labels)\n",
    "    \n",
    "    # Arr of 20 elems\n",
    "    likelihood_arr = np.zeros(len(label_count))\n",
    "    \n",
    "    print('Feature vector')\n",
    "    print(feature_vector)\n",
    "    one_minus_feature_vector = np.add(np.multiply(feature_vector, -1), 1)\n",
    "    \n",
    "    print('1 - Feature vector')\n",
    "    print(one_minus_feature_vector)\n",
    "    \n",
    "    print('word_prob')\n",
    "    print(word_prob[0])\n",
    "    \n",
    "    print('1 - word_prob')\n",
    "    one_minus_word_prob = np.add(np.multiply(word_prob, -1), 1)\n",
    "#     for elem in one_minus_word_prob[19]:\n",
    "#         print(elem)\n",
    "#     print(one_minus_word_prob[0])\n",
    "    \n",
    "    # Logs\n",
    "    log_word_prob = np.log(word_prob)\n",
    "    print('log_word_prob')\n",
    "    print(log_word_prob[0])\n",
    "    \n",
    "    # NEGATIVE VALUES IN LOG\n",
    "        \n",
    "    log_one_minus_word_prob = np.log(one_minus_word_prob)\n",
    "    print('log_one_minus_word_prob')\n",
    "#     for elem in log_one_minus_word_prob[19]:\n",
    "#         print(elem)\n",
    "#         print(log_one_minus_word_prob[0])\n",
    "    \n",
    "    for label in label_probability:\n",
    "        pi_log_prob = np.log(label_probability[label])\n",
    "        if label == 19 or label == 20:\n",
    "            print('pi_log_prob')\n",
    "            print(pi_log_prob)\n",
    "            \n",
    "        dot_product_one = np.dot(feature_vector, log_word_prob[label-1])\n",
    "        if label == 19 or label == 20:\n",
    "            print('dot_product_one')\n",
    "            print(dot_product_one)\n",
    "        \n",
    "        dot_product_two = np.dot(one_minus_feature_vector, log_one_minus_word_prob[label-1])\n",
    "        if label == 19 or label == 20:\n",
    "            print(log_one_minus_word_prob[label-1])\n",
    "            print('dot_product_two')\n",
    "            print(dot_product_two)\n",
    "        \n",
    "        likelihood_arr[label - 1] = pi_log_prob + dot_product_one + dot_product_two\n",
    "        \n",
    "    print(likelihood_arr)\n",
    "    # AGAIN, add 1 to get label from index\n",
    "    return np.argmax(likelihood_arr) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "1 - Feature vector\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "word_prob\n",
      "[0.01244813 0.08091286 0.17219917 ... 0.00207469 0.00207469 0.00207469]\n",
      "1 - word_prob\n",
      "log_word_prob\n",
      "[-4.38618464 -2.51438247 -1.75910351 ... -6.17794411 -6.17794411\n",
      " -6.17794411]\n",
      "log_one_minus_word_prob\n",
      "pi_log_prob\n",
      "-3.189926319726094\n",
      "dot_product_one\n",
      "-38.7538658138926\n",
      "[-0.00214823 -0.0688396  -0.00214823 ... -0.00214823 -0.00214823\n",
      " -0.00214823]\n",
      "dot_product_two\n",
      "-331.2225635420257\n",
      "pi_log_prob\n",
      "-3.400221728562455\n",
      "dot_product_one\n",
      "-38.401284278883985\n",
      "[-0.00264901 -0.08845542 -0.02409755 ... -0.00264901 -0.00264901\n",
      " -0.00264901]\n",
      "dot_product_two\n",
      "-334.62731970018376\n",
      "[-338.79766664 -242.81857707 -246.06343715 -247.65848721 -244.7364244\n",
      " -261.80117251 -225.89305206 -273.6445026  -265.29455211 -266.27006394\n",
      " -285.52770531 -326.46372171 -253.38570367 -297.52491586 -291.91461697\n",
      " -337.14562134 -343.45268277 -370.12750408 -373.16635568 -376.42882571]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_news_data('news.mat')\n",
    "\n",
    "word_list = create_dictionary('news.vocab')\n",
    "\n",
    "word_prob = calculate_word_given_label_probability(train_data, train_labels, word_list)\n",
    "\n",
    "# for elem in word_prob[19]:\n",
    "#     print(elem)\n",
    "\n",
    "# print(check_word_probability(word_prob))\n",
    "\n",
    "predict(train_data[1000], word_prob, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(([1,2], [3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 1. ],\n",
       "       [1.5, 2. ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.divide(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0] = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1.],\n",
       "       [ 1.,  1.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(np.multiply(y, -1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
