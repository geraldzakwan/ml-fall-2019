{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def load_news_data(filepath):\n",
    "    news = loadmat(filepath)\n",
    "\n",
    "    # From scipy csc matrix to 2D array\n",
    "    train_data = news['data'].toarray()\n",
    "    # From 2D array to 1D array\n",
    "    train_labels = news['labels'].flatten()\n",
    "\n",
    "    test_data = news['testdata'].toarray()\n",
    "    test_labels = news['testlabels'].flatten()\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def create_dictionary(filepath='news.vocab'):\n",
    "    with open(filepath, 'r') as f:\n",
    "        list_of_words = f.readlines()\n",
    "\n",
    "    return list_of_words\n",
    "\n",
    "# This is going to be the pi_y, i.e. label probability\n",
    "def calculate_label_count_and_probability(labels):\n",
    "    # Count label occurrence, store in 1D array\n",
    "    # Set array size equals to the number of unique labels\n",
    "    label_count = np.zeros(len(np.unique(labels)))\n",
    "    for label in labels:\n",
    "        # The important thing is to set index to (label - 1)\n",
    "        # as index starts from zero but label starts from 1\n",
    "        label_count[label - 1] = label_count[label - 1] + 1\n",
    "\n",
    "    # Calculate label probability, store in 1D array\n",
    "    label_prob = np.zeros(len(label_count))\n",
    "    for label in labels:\n",
    "        # Again, index is label - 1\n",
    "        label_prob[label - 1] = label_count[label - 1] / len(labels)\n",
    "\n",
    "    return label_count, label_prob\n",
    "\n",
    "# This is going to be the miu_y_j\n",
    "# The idea is to calculate separately for each value of y, i.e.\n",
    "# getting train_data only for a particular value of y\n",
    "# Then, sum over the word index and divide by the number of data\n",
    "def calculate_word_given_label_prob(train_data, train_labels, label_count, word_list, is_sorted, laplace_smoothing=True):\n",
    "    # The return array of shape (20, 61188)\n",
    "    # i.e. (number of labels, word vocab size)\n",
    "    word_prob = np.zeros((len(label_count), len(word_list)))\n",
    "\n",
    "    # Iterate over label\n",
    "    for i in range(0, len(label_count)):\n",
    "        # For each label, find all indexes in train_labels that correspond\n",
    "        indexes, = np.where(train_labels == i + 1) # Again, label is index plus one\n",
    "\n",
    "        # Get the corresponding train_data for that label\n",
    "        if is_sorted:\n",
    "            # If the train_labels is sorted, we can just find two indexes\n",
    "            # where a particular label starts and ends\n",
    "            # Then, use that two indexes to slice the train data\n",
    "            # This is the case for 'news.mat'\n",
    "            first_idx = indexes[0]\n",
    "            last_idx = indexes[-1]\n",
    "\n",
    "            corr_train_data = train_data[first_idx:last_idx]\n",
    "        else:\n",
    "            # If the train_labels is not sorted, we need to use np.take\n",
    "            # to take all train_data that correspond to the label\n",
    "            # This is the case for 'news_binary.mat' and this is rather slow\n",
    "            corr_train_data = np.take(train_data, indexes, axis=0)\n",
    "\n",
    "        # Sum over axis=0, i.e. sum the word occurrence\n",
    "        word_sum = np.sum(corr_train_data, axis=0)\n",
    "\n",
    "        if laplace_smoothing:\n",
    "            # Laplace smoothing, add each sum by 1\n",
    "            word_sum = np.add(word_sum, 1)\n",
    "\n",
    "        # Finally, calculate the word prob for this particular label\n",
    "        if laplace_smoothing:\n",
    "            # Divide by label_count + 2 (Laplace smoothing)\n",
    "            word_prob_for_label = np.divide(word_sum, label_count[i] + 2)\n",
    "        else:\n",
    "            word_prob_for_label = np.divide(word_sum, label_count[i])\n",
    "\n",
    "        # Assign result to the return array\n",
    "        word_prob[i] = word_prob_for_label\n",
    "\n",
    "    return word_prob\n",
    "\n",
    "# Return True if array of probabilities sums up (closely) to 1\n",
    "def check_sum_probability(array, epsilon=0.000001):\n",
    "    return abs(np.sum(array) - 1.0) < epsilon\n",
    "\n",
    "# Input: Feature vectors, 2D array of shape (n, d),\n",
    "# where n is the number of data supplied and d is the dimension: 61188\n",
    "# Return 1D array of predicted labels (size n)\n",
    "def predict(feature_vectors, label_prob, word_prob):\n",
    "    # Create the term 1 - x_j\n",
    "    # Shape: (n, 61188)\n",
    "    one_minus_feature_vectors = np.add(np.multiply(feature_vectors, -1), 1)\n",
    "\n",
    "    # Create the term 1 - miu_y_j\n",
    "    # Shape: (20, 61188)\n",
    "    one_minus_word_prob = np.add(np.multiply(word_prob, -1), 1)\n",
    "\n",
    "    # Log values of all the components\n",
    "    pi_log_prob = np.log(label_prob) # Shape: (20,)\n",
    "    log_word_prob = np.log(word_prob) # Shape: (20, 61188)\n",
    "    log_one_minus_word_prob = np.log(one_minus_word_prob) # Shape: (20, 61188)\n",
    "\n",
    "    # Dot product of x_j and ln(miu_y_j)\n",
    "    # Shape: (n, 20)\n",
    "    dot_product_one = np.dot(feature_vectors, log_word_prob.transpose())\n",
    "\n",
    "    # Dot product of (1 - x_j) and ln(1 - miu_y_j)\n",
    "    # Shape: (n, 20)\n",
    "    dot_product_two = np.dot(one_minus_feature_vectors, log_one_minus_word_prob.transpose())\n",
    "\n",
    "    # pi_log_prob will be broadcasted automatically,\n",
    "    # i.e. from shape (20,) to (n, 20)\n",
    "    # Shape: (n, 20)\n",
    "    final_log_probs = dot_product_one + dot_product_two + pi_log_prob\n",
    "    \n",
    "    print(final_log_probs)\n",
    "\n",
    "    # Finally, return the label that has maximum logprob in each row\n",
    "    # This is done by using argmax on axis=1\n",
    "    # Add the argmax index result by 1 to obtain the correct label\n",
    "    return np.add(np.argmax(final_log_probs, axis=1), 1) # Shape: (n,)\n",
    "\n",
    "def compute_error_rate(test_data, test_labels, label_prob, word_prob):\n",
    "    pred_result = predict(test_data, label_prob, word_prob)\n",
    "\n",
    "    # We compute the error rate here, so wrong prediction will yield 1\n",
    "    # and correct prediction will yield 0\n",
    "    pred_verdict = [1 if pred_result[i] != test_labels[i] else 0 for i in range(0, len(test_data))]\n",
    "\n",
    "    # Sum the wrong predictions and divide it by total test data\n",
    "    return np.sum(pred_verdict) / len(pred_verdict)\n",
    "\n",
    "# Experiment for 20 labels\n",
    "def experiment_3a():\n",
    "    start = time.time()\n",
    "\n",
    "    print('Experiment 3a: ')\n",
    "    print()\n",
    "\n",
    "    # Get the data\n",
    "    train_data, train_labels, test_data, test_labels = load_news_data('news.mat')\n",
    "\n",
    "    # Create word vocab list\n",
    "    word_list = create_dictionary()\n",
    "\n",
    "    # Calculate pi_y, i.e. the label probability\n",
    "    label_count, label_prob = calculate_label_count_and_probability(train_labels)\n",
    "\n",
    "    # Sanity check (probabilities sum up to 1)\n",
    "    assert check_sum_probability(label_prob)\n",
    "\n",
    "    # Calculate miu_y_j, i.e. the word probability\n",
    "    word_prob = calculate_word_given_label_prob(train_data, train_labels, label_count, word_list, True)\n",
    "    print('Done calculating word prob')\n",
    "    print('Elapsed time: ' + str(time.time() - start))\n",
    "    print()\n",
    "\n",
    "    # Calculate train_error_rate\n",
    "    train_error_rate = compute_error_rate(train_data, train_labels, label_prob, word_prob)\n",
    "    print('Train error rate: ' + str(train_error_rate))\n",
    "    print('Done calculating train error rate')\n",
    "    print('Elapsed time: ' + str(time.time() - start))\n",
    "    print()\n",
    "\n",
    "    # Calculate test_error_rate\n",
    "    test_error_rate = compute_error_rate(test_data, test_labels, label_prob, word_prob)\n",
    "    print('Test error rate: ' + str(test_error_rate))\n",
    "    print('Done calculating test error rate')\n",
    "    print('Elapsed time: ' + str(time.time() - start))\n",
    "    print()\n",
    "\n",
    "    print('----------------------------------------')\n",
    "    print()\n",
    "\n",
    "# Experiment for binary labels\n",
    "def experiment_3b():\n",
    "    start = time.time()\n",
    "\n",
    "    print('Experiment 3b: ')\n",
    "    print()\n",
    "\n",
    "    # Get the data\n",
    "    train_data, train_labels, test_data, test_labels = load_news_data('news_binary.mat')\n",
    "\n",
    "    # Modify label from (-1, 1) to (1, 2) so it's easier to process\n",
    "    # as it follows the previous convention that I use: label = index + 1\n",
    "    train_labels = np.where(train_labels == -1, 1, 2)\n",
    "    test_labels = np.where(test_labels == -1, 1, 2)\n",
    "\n",
    "    # Create word vocab list\n",
    "    word_list = create_dictionary()\n",
    "\n",
    "    # Calculate pi_y, i.e. the label probability\n",
    "    label_count, label_prob = calculate_label_count_and_probability(train_labels)\n",
    "\n",
    "    # Sanity check (probabilities sum up to 1)\n",
    "    assert check_sum_probability(label_prob)\n",
    "\n",
    "    # Calculate miu_y_j, i.e. the word probability\n",
    "    # The difference from experiment 3a is than the train_labels is not sorted\n",
    "    word_prob = calculate_word_given_label_prob(train_data, train_labels, label_count, word_list, False)\n",
    "    print('Done calculating word prob')\n",
    "    print('Elapsed time: ' + str(time.time() - start))\n",
    "    print()\n",
    "\n",
    "    # Calculate train_error_rate\n",
    "    train_error_rate = compute_error_rate(train_data, train_labels, label_prob, word_prob)\n",
    "    print('Train error rate: ' + str(train_error_rate))\n",
    "    print('Done calculating train error rate')\n",
    "    print('Elapsed time: ' + str(time.time() - start))\n",
    "    print()\n",
    "\n",
    "    # Calculate test_error_rate\n",
    "    test_error_rate = compute_error_rate(test_data, test_labels, label_prob, word_prob)\n",
    "    print('Test error rate: ' + str(test_error_rate))\n",
    "    print('Done calculating test error rate')\n",
    "    print('Elapsed time: ' + str(time.time() - start))\n",
    "    print()\n",
    "\n",
    "    print('----------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3b: \n",
      "\n",
      "Done calculating word prob\n",
      "Elapsed time: 11.52760910987854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "print('Experiment 3b: ')\n",
    "print()\n",
    "\n",
    "# Get the data\n",
    "train_data, train_labels, test_data, test_labels = load_news_data('news_binary.mat')\n",
    "\n",
    "# Modify label from (-1, 1) to (1, 2) so it's easier to process\n",
    "# as it follows the previous convention that I use: label = index + 1\n",
    "train_labels = np.where(train_labels == -1, 1, 2)\n",
    "test_labels = np.where(test_labels == -1, 1, 2)\n",
    "\n",
    "# Create word vocab list\n",
    "word_list = create_dictionary()\n",
    "\n",
    "# Calculate pi_y, i.e. the label probability\n",
    "label_count, label_prob = calculate_label_count_and_probability(train_labels)\n",
    "\n",
    "# Sanity check (probabilities sum up to 1)\n",
    "assert check_sum_probability(label_prob)\n",
    "\n",
    "# Calculate miu_y_j, i.e. the word probability\n",
    "# The difference from experiment 3a is than the train_labels is not sorted\n",
    "word_prob = calculate_word_given_label_prob(train_data, train_labels, label_count, word_list, False)\n",
    "print('Done calculating word prob')\n",
    "print('Elapsed time: ' + str(time.time() - start))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -340.78240945  -336.80985632]\n",
      " [-1705.98589787 -1794.1743824 ]\n",
      " [ -504.87991791  -453.24326971]\n",
      " [-1134.03846954 -1055.77855955]\n",
      " [ -429.2539048   -497.0267632 ]\n",
      " [-1920.82221312 -1724.00185736]\n",
      " [ -423.14698729  -391.62096716]\n",
      " [-1024.23833212  -944.79000238]\n",
      " [ -318.83883829  -317.24375706]\n",
      " [ -730.95131555  -652.95896001]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict(train_data[0].reshape(-1, 1).transpose(), label_prob, word_prob)\n",
    "predict(train_data[0:10], label_prob, word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00411805, 0.1002059 , 0.07137955, ..., 0.00068634, 0.00068634,\n",
       "        0.00068634],\n",
       "       [0.00698413, 0.08126984, 0.00063492, ..., 0.00063492, 0.00063492,\n",
       "        0.00063492]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias(word_prob):\n",
    "    log_label_prob = np.log(label_prob)\n",
    "    label_prob_diff = log_label_prob[1] - log_label_prob[0]\n",
    "    \n",
    "    one_minus_word_prob = np.add(np.multiply(word_prob, -1), 1)\n",
    "    log_one_minus_word_prob = np.log(one_minus_word_prob)\n",
    "    word_prob_diff = np.sum(log_one_minus_word_prob[1]) - np.sum(log_one_minus_word_prob[0])\n",
    "    \n",
    "    return label_prob_diff + word_prob_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.78486435545388"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bias(word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(word_prob):\n",
    "    one_minus_word_prob = np.add(np.multiply(word_prob, -1), 1)\n",
    "    \n",
    "    log_word_prob = np.log(word_prob)\n",
    "    log_one_minus_word_prob = np.log(one_minus_word_prob)\n",
    "    \n",
    "    return (log_word_prob[1] - log_word_prob[0]) - (log_one_minus_word_prob[1] - log_one_minus_word_prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53114214, -0.2302786 , -4.7956867 , ..., -0.0779272 ,\n",
       "       -0.0779272 , -0.0779272 ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_weights(word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_affine(feature_vectors, word_prob):\n",
    "    wx = np.dot(feature_vectors, calculate_weights(word_prob))\n",
    "    b = calculate_bias(word_prob)\n",
    "    \n",
    "    y = wx + b \n",
    "     \n",
    "    return [2 if pred > 0 else 1 for pred in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 2, 1, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(predict_affine(train_data[0], word_prob))\n",
    "# print(predict_affine(train_data[1], word_prob))\n",
    "# print(predict_affine(train_data[2], word_prob))\n",
    "# print(predict_affine(train_data[3], word_prob))\n",
    "# print(predict_affine(train_data[4], word_prob))\n",
    "# print(predict_affine(train_data[5], word_prob))\n",
    "# print(predict_affine(train_data[6], word_prob))\n",
    "# print(predict_affine(train_data[7], word_prob))\n",
    "# print(predict_affine(train_data[8], word_prob))\n",
    "# print(predict_affine(train_data[9], word_prob))\n",
    "\n",
    "predict_affine(train_data, word_prob)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -340.78240945  -336.80985632]\n",
      " [-1705.98589787 -1794.1743824 ]\n",
      " [ -504.87991791  -453.24326971]\n",
      " [-1134.03846954 -1055.77855955]\n",
      " [ -429.2539048   -497.0267632 ]\n",
      " [-1920.82221312 -1724.00185736]\n",
      " [ -423.14698729  -391.62096716]\n",
      " [-1024.23833212  -944.79000238]\n",
      " [ -318.83883829  -317.24375706]\n",
      " [ -730.95131555  -652.95896001]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict(train_data[0].reshape(-1, 1).transpose(), label_prob, word_prob)\n",
    "predict(train_data[0:10], label_prob, word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x10b424288>\n"
     ]
    }
   ],
   "source": [
    "print(zip([1,2], [3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
