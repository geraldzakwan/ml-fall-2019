{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def load_news_data(filepath='news.mat'):\n",
    "    news = loadmat(filepath)\n",
    "\n",
    "    # From scipy csc matrix to 2D array\n",
    "    train_data = news['data'].toarray()\n",
    "    # From 2D array to 1D array\n",
    "    train_labels = news['labels'].flatten()\n",
    "\n",
    "    test_data = news['testdata'].toarray()\n",
    "    test_labels = news['testlabels'].flatten()\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def create_dictionary(filepath='news.vocab'):\n",
    "    with open(filepath, 'r') as f:\n",
    "        list_of_words = f.readlines()\n",
    "\n",
    "    return list_of_words\n",
    "\n",
    "# This is going to be the pi_y, i.e. label probability\n",
    "def calculate_label_count_and_probability(labels):\n",
    "    # Count label occurrence, store in 1D array\n",
    "    # Set array size equals to the number of unique labels\n",
    "    label_count = np.zeros(len(np.unique(labels)))\n",
    "    for label in labels:\n",
    "        # The important thing is to set index to (label - 1)\n",
    "        # as index starts from zero but label starts from 1\n",
    "        label_count[label - 1] = label_count[label - 1] + 1\n",
    "\n",
    "    # Calculate label probability, store in 1D array\n",
    "    label_prob = np.zeros(len(label_count))\n",
    "    for label in labels:\n",
    "        # Again, index is label - 1\n",
    "        label_prob[label - 1] = label_count[label - 1] / len(labels)\n",
    "\n",
    "    return label_count, label_prob\n",
    "\n",
    "# This is going to be the miu_y_j\n",
    "# The idea is to calculate separately for each value of y, i.e.\n",
    "# getting train_data only for a particular value of y\n",
    "# Then, sum over the word index and divide by the number of data\n",
    "def calculate_word_given_label_prob(train_data, train_labels, label_count, word_list, laplace_smoothing=True):\n",
    "    # The return array of shape (20, 61188)\n",
    "    # i.e. (number of labels, word vocab size)\n",
    "    word_prob = np.zeros((len(label_count), len(word_list)))\n",
    "\n",
    "    # Iterate over label\n",
    "    for i in range(0, len(label_count)):\n",
    "        # I observe that train_labels is sorted ascendingly (1 to 20)\n",
    "        # So, we can just find two indexes where a particular label starts and ends\n",
    "        # This is done using the following 3 lines\n",
    "        idx, = np.where(train_labels == i + 1) # Again, label is index plus one\n",
    "        first_idx = idx[0]\n",
    "        last_idx = idx[-1]\n",
    "        \n",
    "        print(first_idx)\n",
    "        print(last_idx)\n",
    "\n",
    "        # Get the corresponding train_data for that label\n",
    "        corr_train_data = train_data[first_idx:last_idx]\n",
    "\n",
    "        # Sum over axis=0, i.e. sum the word occurrence\n",
    "        word_sum = np.sum(corr_train_data, axis=0)\n",
    "\n",
    "        if laplace_smoothing:\n",
    "            # Laplace smoothing, add each sum by 1\n",
    "            word_sum = np.add(word_sum, 1)\n",
    "\n",
    "        # Finally, calculate the word prob for this particular label\n",
    "        if laplace_smoothing:\n",
    "            # Divide by label_count + 2 (Laplace smoothing)\n",
    "            word_prob_for_label = np.divide(word_sum, label_count[i] + 2)\n",
    "        else:\n",
    "            word_prob_for_label = np.divide(word_sum, label_count[i])\n",
    "\n",
    "        # Assign result to the return array\n",
    "        word_prob[i] = word_prob_for_label\n",
    "\n",
    "    return word_prob\n",
    "\n",
    "# Return True if array of probabilities sums up (closely) to 1\n",
    "def check_sum_probability(array, epsilon=0.000001):\n",
    "    return abs(np.sum(array) - 1.0) < epsilon\n",
    "\n",
    "# Input: Feature vectors, 2D array of shape (n, d),\n",
    "# where n is the number of data supplied and d is the dimension: 61188\n",
    "# Return 1D array of predicted labels (size n)\n",
    "def predict(feature_vectors, label_prob, word_prob):\n",
    "    # Create the term 1 - x_j\n",
    "    # Shape: (n, 61188)\n",
    "    one_minus_feature_vectors = np.add(np.multiply(feature_vectors, -1), 1)\n",
    "\n",
    "    # Create the term 1 - miu_y_j\n",
    "    # Shape: (20, 61188)\n",
    "    one_minus_word_prob = np.add(np.multiply(word_prob, -1), 1)\n",
    "\n",
    "    # Log values of all the components\n",
    "    pi_log_prob = np.log(label_prob) # Shape: (20,)\n",
    "    log_word_prob = np.log(word_prob) # Shape: (20, 61188)\n",
    "    log_one_minus_word_prob = np.log(one_minus_word_prob) # Shape: (20, 61188)\n",
    "\n",
    "    # Dot product of x_j and ln(miu_y_j)\n",
    "    # Shape: (n, 20)\n",
    "    dot_product_one = np.dot(feature_vectors, log_word_prob.transpose())\n",
    "\n",
    "    # Dot product of (1 - x_j) and ln(1 - miu_y_j)\n",
    "    # Shape: (n, 20)\n",
    "    dot_product_two = np.dot(one_minus_feature_vectors, log_one_minus_word_prob.transpose())\n",
    "\n",
    "    # pi_log_prob will be broadcasted automatically,\n",
    "    # i.e. from shape (20,) to (n, 20)\n",
    "    # Shape: (n, 20)\n",
    "    final_log_probs = dot_product_one + dot_product_two + pi_log_prob\n",
    "\n",
    "    # Finally, return the label that has maximum logprob in each row\n",
    "    # This is done by using argmax on axis=1\n",
    "    # Add the argmax index result by 1 to obtain the correct label\n",
    "    return np.add(np.argmax(final_log_probs, axis=1), 1) # Shape: (n,)\n",
    "\n",
    "def compute_error_rate(test_data, test_labels, label_prob, word_prob):\n",
    "    pred_result = predict(test_data, label_prob, word_prob)\n",
    "\n",
    "    # We compute the error rate here, so wrong prediction will yield 1\n",
    "    # and correct prediction will yield 0\n",
    "    pred_verdict = [1 if pred_result[i] != test_labels[i] else 0 for i in range(0, len(test_data))]\n",
    "\n",
    "    # Sum the wrong predictions and divide it by total test data\n",
    "    return np.sum(pred_verdict) / len(pred_verdict)\n",
    "\n",
    "# Experiment for 20 labels\n",
    "def experiment_3a():\n",
    "    print('EXPERIMENT 3A: ')\n",
    "    print()\n",
    "\n",
    "    # Get the data\n",
    "    train_data, train_labels, test_data, test_labels = load_news_data()\n",
    "\n",
    "    # Create word vocab list\n",
    "    word_list = create_dictionary()\n",
    "\n",
    "    # Calculate pi_y, i.e. the label probability\n",
    "    label_count, label_prob = calculate_label_count_and_probability(train_labels)\n",
    "\n",
    "    # Sanity check (probabilities sum up to 1)\n",
    "    assert check_sum_probability(label_prob)\n",
    "\n",
    "    # Calculate miu_y_j, i.e. the word probability\n",
    "    word_prob = calculate_word_given_label_prob(train_data, train_labels, label_count, word_list)\n",
    "\n",
    "    # Calculate train_error_rate\n",
    "    train_error_rate = compute_error_rate(train_data, train_labels, label_prob, word_prob)\n",
    "    print('Train error rate: ' + str(train_error_rate))\n",
    "\n",
    "    # Calculate test_error_rate\n",
    "    test_error_rate = compute_error_rate(test_data, test_labels, label_prob, word_prob)\n",
    "    print('Test error rate: ' + str(test_error_rate))\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "binary_train_data, binary_train_labels, binary_test_data, binary_test_labels = load_news_data('news_binary.mat')\n",
    "\n",
    "# Create word vocab list\n",
    "word_list = create_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1  1 -1  1  1  1  1  1]\n",
      "[2 1 2 2 1 2 2 2 2 2]\n",
      "(3028, 61188)\n"
     ]
    }
   ],
   "source": [
    "# Modify label from (-1, 1) to (1, 2) so it's easier to process \n",
    "# as it follows the previous convention that I use: label = index + 1\n",
    "print(binary_train_labels[0:10])\n",
    "binary_train_labels = np.where(binary_train_labels == -1, 1, 2)\n",
    "print(binary_train_labels[0:10])\n",
    "binary_test_labels = np.where(binary_test_labels == -1, 1, 2)\n",
    "\n",
    "print(binary_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pi_y, i.e. the label probability\n",
    "binary_label_count, binary_label_prob = calculate_label_count_and_probability(binary_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check (probabilities sum up to 1)\n",
    "assert check_sum_probability(binary_label_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3026\n",
      "0\n",
      "3027\n"
     ]
    }
   ],
   "source": [
    "# Calculate miu_y_j, i.e. the word probability\n",
    "binary_word_prob = calculate_word_given_label_prob(binary_train_data, binary_train_labels, binary_label_count, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0034317089910774"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(binary_word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: 0.5194848084544254\n"
     ]
    }
   ],
   "source": [
    "# Calculate train_error_rate\n",
    "binary_train_error_rate = compute_error_rate(binary_train_data, binary_train_labels, binary_label_prob, binary_word_prob)\n",
    "print('Train error rate: ' + str(binary_train_error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate: 0.5205751115518096\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate test_error_rate\n",
    "binary_test_error_rate = compute_error_rate(binary_test_data, binary_test_labels, binary_label_prob, binary_word_prob)\n",
    "print('Test error rate: ' + str(binary_test_error_rate))\n",
    "\n",
    "print('----------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array(([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(z, [0, 1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
